{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "omxjI7shcBaP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AgIFpnlCb3vv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\benja\\Documents\\Universidad\\2025-2\\Recomendadores\\Proyecto\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import surprise\n",
        "import implicit\n",
        "from surprise import accuracy\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import PredefinedKFold\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGRpQPLrTdAd"
      },
      "source": [
        "Dataset original de https://www.kaggle.com/datasets/marlesson/myanimelist-dataset-animes-profiles-reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1142
        },
        "id": "xH122FhZNZQD",
        "outputId": "7d7ca6f9-69a7-4d61-9124-afb08b9b1881"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'animes.csv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m animes = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manimes.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m animes.head()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Universidad\\2025-2\\Recomendadores\\Proyecto\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Universidad\\2025-2\\Recomendadores\\Proyecto\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Universidad\\2025-2\\Recomendadores\\Proyecto\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Universidad\\2025-2\\Recomendadores\\Proyecto\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\benja\\Documents\\Universidad\\2025-2\\Recomendadores\\Proyecto\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'animes.csv'"
          ]
        }
      ],
      "source": [
        "animes = pd.read_csv(\"animes.csv\")\n",
        "animes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "JA2J1NRHUAX1",
        "outputId": "d13add68-b247-4d25-90be-da6f1083e22f"
      },
      "outputs": [],
      "source": [
        "profiles = pd.read_csv(\"profiles.csv\")\n",
        "profiles.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Tj6Dyf3JOVNF",
        "outputId": "91fe1688-f970-4d2b-f0f2-ea1679d3e719"
      },
      "outputs": [],
      "source": [
        "reviews = pd.read_csv(\"reviews.csv\")\n",
        "\n",
        "# Assign a unique id to each username\n",
        "unique_users = reviews[\"username\"].unique()\n",
        "user_mapping = {user: i for i, user in enumerate(unique_users)}\n",
        "reviews[\"user_id\"] = reviews[\"username\"].map(user_mapping)\n",
        "\n",
        "# Assign anime uid from animes df (join on reviews.anime = animes.title)\n",
        "anime_ids = animes[['title', 'uid']].copy()\n",
        "reviews = pd.merge(reviews, anime_ids,\n",
        "                   left_on='anime',\n",
        "                   right_on='title',\n",
        "                   how='left')\n",
        "reviews = reviews.drop(columns=['title'])\n",
        "reviews = reviews.rename(columns={'uid': 'anime_uid'})\n",
        "\n",
        "reviews.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef0_bNBXMWgw"
      },
      "source": [
        "# Análisis de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WypgnvLhYpGG"
      },
      "outputs": [],
      "source": [
        "reviews = reviews.rename(columns={\"anime_uid\": \"item_id\", \"score\": \"rating\"})\n",
        "# drop rows whose item_id is not in animes\n",
        "# reviews = reviews[reviews[\"item_id\"].isin(animes[\"uid\"])]\n",
        "# drop rows whose user_id is not in profiles\n",
        "# reviews = reviews[reviews[\"user_id\"].isin(profiles[\"uid\"])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM9ctH1dWy8Q"
      },
      "source": [
        "Quitar nacimiento que no tiene año"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jdiFBHBUbuU",
        "outputId": "d643bfa0-b046-4ca7-b858-7fac3d97e48e"
      },
      "outputs": [],
      "source": [
        "profiles[\"birthday_year\"] = profiles[\"birthday\"].apply(lambda x: x[-4:] if pd.notna(x) else None)\n",
        "profiles = profiles[profiles[\"birthday_year\"].apply(lambda x: len(x) == 4 and x.isdigit() if pd.notna(x) else True)]\n",
        "print(profiles[\"birthday_year\"].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl4limo4W7rZ"
      },
      "outputs": [],
      "source": [
        "reviews_with_more_than_one = reviews.groupby(\"user_id\").filter(lambda x: len(x) > 1)\n",
        "reviews_with_more_than_one.head()\n",
        "train_file, validation_file = train_test_split(reviews_with_more_than_one, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cOAqw9PMbAQ",
        "outputId": "2a46a9f1-3d2b-44b1-aa5e-35b98d7a200a"
      },
      "outputs": [],
      "source": [
        "def get_stats(df):\n",
        "    users_quantity = df[\"user_id\"].nunique()\n",
        "    items_quantity = df[\"item_id\"].nunique()\n",
        "\n",
        "    total_ratings = len(df)\n",
        "    avg_ratings_per_user = total_ratings / users_quantity\n",
        "    avg_ratings_per_item = total_ratings / items_quantity\n",
        "\n",
        "    avg_rating = df[\"rating\"].mean()\n",
        "    std_rating = df[\"rating\"].std()\n",
        "    highest_number_of_ratings_by_user = df.groupby(\"user_id\").size().max()\n",
        "    highest_number_of_ratings_for_item = df.groupby(\"item_id\").size().max()\n",
        "\n",
        "    density = (total_ratings / (users_quantity * items_quantity)) * 100\n",
        "\n",
        "    return {\n",
        "        \"Número de usuarios\": users_quantity,\n",
        "        \"Número de animes\": items_quantity,\n",
        "        \"Total de ratings\": total_ratings,\n",
        "        \"Media de ratings por usuario\": avg_ratings_per_user,\n",
        "        \"Media de ratings por anime\": avg_ratings_per_item,\n",
        "        \"Rating media\": avg_rating,\n",
        "        \"Desviación estándar de ratings\": std_rating,\n",
        "        \"Mayor número de ratings por un usuario\": highest_number_of_ratings_by_user,\n",
        "        \"Mayor número de ratings para un anime\": highest_number_of_ratings_for_item,\n",
        "        \"Densidad (%)\": f\"{density}%\"\n",
        "    }\n",
        "\n",
        "train_stats = get_stats(train_file)\n",
        "val_stats = get_stats(validation_file)\n",
        "\n",
        "rows = []\n",
        "for key in train_stats.keys():\n",
        "    rows.append([key, train_stats[key], val_stats[key]])\n",
        "\n",
        "print(tabulate(rows, headers=[\"Estadística\", \"Training\", \"Validation\"], tablefmt=\"grid\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "nezHG1E2Mehz",
        "outputId": "cf48f571-edca-4308-d7d8-c5f5f8b590c4"
      },
      "outputs": [],
      "source": [
        "def plot_rating_distribution(ratings, title):\n",
        "    counts = ratings.value_counts()\n",
        "    bars = plt.bar(counts.index, counts.values)\n",
        "    for bar, count in zip(bars, counts.values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                 str(count), ha='center', va='bottom')\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Rating\")\n",
        "    plt.ylabel(\"Frecuencia\")\n",
        "    plt.grid(axis='y')\n",
        "    plt.show()\n",
        "\n",
        "plot_rating_distribution(train_file[\"rating\"], \"Distribución de Ratings - Training\")\n",
        "plot_rating_distribution(validation_file[\"rating\"], \"Distribución de Ratings - Validation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "B07hzH8QMgsA",
        "outputId": "06c2202f-d212-40bc-80e1-0250794f8a3b"
      },
      "outputs": [],
      "source": [
        "MAX_RATINGS = 5\n",
        "\n",
        "def plot_ratings_per_user_distribution(df, title):\n",
        "    ratings_per_user = df.groupby(\"user_id\").size()\n",
        "    clipped = ratings_per_user.clip(upper=MAX_RATINGS)\n",
        "    counts = clipped.value_counts().reindex(range(0, MAX_RATINGS+1), fill_value=0)\n",
        "    counts.index = [str(i) if i < MAX_RATINGS else f\"{MAX_RATINGS}+\" for i in range(0, MAX_RATINGS+1)]\n",
        "\n",
        "    bars = plt.bar(counts.index, counts.values, width=0.9, edgecolor=\"black\")\n",
        "    for bar, count in zip(bars, counts.values):\n",
        "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
        "                 str(count), ha=\"center\", va=\"bottom\", fontsize=8)\n",
        "\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Número ratings por usuario\")\n",
        "    plt.ylabel(\"Cantidad de usuarios\")\n",
        "    plt.grid(axis=\"y\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_ratings_per_user_distribution(train_file, \"Número de Ratings vs Cantidad de Usuarios - Training\")\n",
        "plot_ratings_per_user_distribution(validation_file, \"Número de Ratings vs Cantidad de Usuarios - Validation\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-NC6tJzb2Qx"
      },
      "source": [
        "### Analísis de genero y edad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "hU0p_Og2ZrZY",
        "outputId": "a750b346-cbae-4569-8303-36c736671e6c"
      },
      "outputs": [],
      "source": [
        "profiles[\"gender\"].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "W7ajRSZdb7qs",
        "outputId": "1645b31f-83e7-4d4a-fb6c-abcc65a07823"
      },
      "outputs": [],
      "source": [
        "profiles[\"birthday_year\"] = profiles[\"birthday_year\"].apply(lambda x: int(x) if pd.notna(x) else None)\n",
        "profiles[\"edad\"] = profiles[\"birthday_year\"].apply(lambda x: 2025 - x if pd.notna(x) else None)\n",
        "profiles[\"edad\"].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "6e14afc3",
        "outputId": "ceaa94aa-205d-4acf-8700-efdc900ed1c4"
      },
      "outputs": [],
      "source": [
        "age_counts = profiles[\"edad\"].value_counts().sort_index()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "bars = plt.bar(age_counts.index, age_counts.values)\n",
        "plt.xlabel(\"Edad\")\n",
        "plt.ylabel(\"Cantidad de Usuarios\")\n",
        "plt.title(\"Distribución de Edad de los Usuarios\")\n",
        "plt.grid(axis='y')\n",
        "\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    if height <= 10:\n",
        "        plt.text(bar.get_x() + bar.get_width() / 2., height,\n",
        "                 '%d' % int(height),\n",
        "                 ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clQ5C04pabZE"
      },
      "source": [
        "# Listas de Recomendaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akc8apvy0Vi4"
      },
      "outputs": [],
      "source": [
        "train_file[['user_id', 'item_id', 'rating']].to_csv(\"train\", index=False, header=False)\n",
        "validation_file[['user_id', 'item_id', 'rating']].to_csv(\"test\", index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0RPvzG9z_bd"
      },
      "outputs": [],
      "source": [
        "reader = surprise.Reader(line_format='user item rating', sep=',', rating_scale=(1, 5), skip_lines=1)\n",
        "data = surprise.Dataset.load_from_folds([('train', 'test')], reader=reader)\n",
        "pkf = PredefinedKFold()\n",
        "trainset, testset = next(pkf.split(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "x-h7dDAdaeOS",
        "outputId": "62998ff0-0f0c-4f28-f09e-ed323da32cba"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(\n",
        "    \"train\", sep=\",\", names=[\"userid\", \"itemid\", \"rating\"], header=None\n",
        ")\n",
        "\n",
        "df_train.rating = [1 if x >= 5 else 0 for x in df_train.rating]\n",
        "\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "do_l2pTxaiki",
        "outputId": "2d8d395c-4524-4e2a-a640-510506cd98eb"
      },
      "outputs": [],
      "source": [
        "df_test = pd.read_csv(\n",
        "    \"test\", sep=\",\", names=[\"userid\", \"itemid\", \"rating\"], header=None\n",
        ")\n",
        "\n",
        "df_test.rating = [1 if x >= 5 else 0 for x in df_test.rating]\n",
        "\n",
        "df_test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HJxMcsaao3Z"
      },
      "outputs": [],
      "source": [
        "user_items_test = {}\n",
        "\n",
        "for row in df_test.itertuples():\n",
        "    if row[1] not in user_items_test:\n",
        "        user_items_test[row[1]] = []\n",
        "\n",
        "    user_items_test[row[1]].append(row[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgEUhNoDaqJM"
      },
      "outputs": [],
      "source": [
        "item_interaction_counts = df_train['itemid'].value_counts()\n",
        "user_count = df_train['userid'].nunique()\n",
        "item_popularity = (item_interaction_counts / user_count).to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V0QqOhfasDp"
      },
      "outputs": [],
      "source": [
        "metadata = animes[['uid', 'genre']]\n",
        "item_categories: dict[int, set[str | None]] = {}\n",
        "for row in metadata.itertuples():\n",
        "    item_categories[int(row[1]) if row[1].is_integer() else row[1]] = set(map(lambda i: i.strip(), row[2].split(','))) if isinstance(row[2], str) else set()\n",
        "\n",
        "def precision_at_k(r, k):\n",
        "  assert 1 <= k <= r.size\n",
        "  return (np.asarray(r)[:k]).mean()\n",
        "\n",
        "\n",
        "def average_precision_at_k(r, k):\n",
        "    r = np.asarray(r)\n",
        "    n_rel = r.sum()\n",
        "    if n_rel == 0:\n",
        "        return 0.0\n",
        "    vectorized_precision = np.vectorize(lambda i: precision_at_k(r, i))\n",
        "    indices = np.arange(1, len(r) + 1)\n",
        "    precisions = vectorized_precision(\n",
        "        indices\n",
        "    )\n",
        "    score = np.sum(precisions * r)\n",
        "    return score / min(k, n_rel)\n",
        "\n",
        "\n",
        "def recall_at_k(r, k):\n",
        "    r = np.asarray(r)\n",
        "    n_rel = r.sum()\n",
        "    if n_rel == 0:\n",
        "        return 0.0\n",
        "    return np.sum(r[:k]) / n_rel\n",
        "\n",
        "\n",
        "def dcg_at_k(r, k):\n",
        "    r = np.asarray(r)[:k]\n",
        "    if r.size:\n",
        "        return np.sum(\n",
        "            np.subtract(np.power(2, r), 1) / np.log2(np.arange(2, r.size + 2))\n",
        "        )\n",
        "    return 0.0\n",
        "\n",
        "\n",
        "def ndcg_at_k(r, k):\n",
        "    idcg = dcg_at_k(sorted(r, reverse=True), k)\n",
        "\n",
        "    if not idcg:\n",
        "        return 0.0\n",
        "    return dcg_at_k(r, k) / idcg\n",
        "\n",
        "\n",
        "def novelty(recommended_items):\n",
        "    score = 0\n",
        "    for item in recommended_items:\n",
        "        popularity = item_popularity.get(item, 0)\n",
        "        score += np.log2(1/popularity) if popularity > 0 else 0\n",
        "    return score / len(recommended_items) if any(recommended_items) else 0\n",
        "\n",
        "\n",
        "def diversity(r, k):\n",
        "    score = 0\n",
        "    for i in range(len(r)):\n",
        "        for j in range(i + 1, len(r)):\n",
        "            item_i = r[i]\n",
        "            item_j = r[j]\n",
        "            categories_i = item_categories.get(item_i, set())\n",
        "            categories_j = item_categories.get(item_j, set())\n",
        "            if categories_i or categories_j:\n",
        "                intersection = categories_i.intersection(categories_j)\n",
        "                union = categories_i.union(categories_j)\n",
        "                jaccard_distance = 1 - (len(intersection) / len(union))\n",
        "                score += jaccard_distance\n",
        "\n",
        "    num_pairs = (k * (k - 1)) / 2\n",
        "    return score / num_pairs if num_pairs > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9n2Oax7aw7B"
      },
      "outputs": [],
      "source": [
        "user_items = {}\n",
        "itemset = set()\n",
        "\n",
        "for row in df_train.itertuples():\n",
        "    if row[1] not in user_items:\n",
        "        user_items[row[1]] = []\n",
        "\n",
        "    user_items[row[1]].append(row[2])\n",
        "    itemset.add(row[2])\n",
        "\n",
        "itemset = np.sort(list(itemset))\n",
        "\n",
        "sparse_matrix = np.zeros((len(user_items), len(itemset)))\n",
        "\n",
        "for i, items in enumerate(user_items.values()):\n",
        "    sparse_matrix[i] = np.isin(itemset, items, assume_unique=True).astype(int)\n",
        "\n",
        "matrix = sp.csr_matrix(sparse_matrix.T)\n",
        "\n",
        "user_item_matrix = matrix.T.tocsr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbhn3vjcayik"
      },
      "outputs": [],
      "source": [
        "user2row = {user_id: matrix_row for matrix_row, user_id in enumerate(user_items.keys())}\n",
        "row2user = {matrix_row: user_id for user_id, matrix_row in user2row.items()}\n",
        "\n",
        "item2col = {item_id: matrix_col for matrix_col, item_id in enumerate(itemset)}\n",
        "col2item = {matrix_col: item_id for item_id, matrix_col in item2col.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQvo8BMm_1ep"
      },
      "outputs": [],
      "source": [
        "animes['popularity_score'] = (\n",
        "                (1 / animes['popularity']) * 0.4 +\n",
        "                (1 / animes['ranked'].fillna(10000)) * 0.3 +\n",
        "                animes['score'] * 0.3\n",
        "            )\n",
        "popular_animes = animes.sort_values('popularity_score', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZZMBOrtazw1"
      },
      "outputs": [],
      "source": [
        "class ModelWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        # if model == \"most_popular\":\n",
        "        #     self.most_popular = AnimeMostPopularRecommender(popularity_metric=\"weighted\")\n",
        "        #     self.most_popular.fit(animes)\n",
        "\n",
        "    def recommend(self, user_id, user_item_matrix, n=10):\n",
        "        if self.model == \"random\":\n",
        "            seen = set(user_items.get(user_id, []))\n",
        "            item_array = np.array(list(itemset))\n",
        "            mask = ~np.isin(item_array, list(seen))\n",
        "            candidates = item_array[mask]\n",
        "            return np.random.choice(candidates, size=n, replace=False)\n",
        "\n",
        "        if self.model == \"most_popular\":\n",
        "            seen = set(user_items.get(user_id, []))\n",
        "            recommendations = []\n",
        "            for _, anime in popular_animes.iterrows():\n",
        "                if anime['uid'] not in seen:\n",
        "                    recommendations.append(anime['uid'])\n",
        "                if len(recommendations) >= n:\n",
        "                    break\n",
        "            return recommendations\n",
        "\n",
        "        if hasattr(self.model, 'recommend') and callable(getattr(self.model, 'recommend')):\n",
        "            try:\n",
        "                user_row = user2row[user_id]\n",
        "                rec = self.model.recommend(user_row, user_item_matrix[user_row], n)[0]\n",
        "                return np.array([col2item[col] for col in rec])\n",
        "            except KeyError:\n",
        "                # Fallback a Most Popular\n",
        "                # print(f\"User {user_id} not in user2row, using most popular fallback\")\n",
        "                seen = set(user_items.get(user_id, []))\n",
        "                recommendations = []\n",
        "                for _, anime in popular_animes.iterrows():\n",
        "                    if anime['uid'] not in seen:\n",
        "                        recommendations.append(anime['uid'])\n",
        "                    if len(recommendations) >= n:\n",
        "                        break\n",
        "                return recommendations\n",
        "\n",
        "        else:\n",
        "            trainset = self.model.trainset\n",
        "            u = trainset.to_inner_uid(str(user_id))\n",
        "\n",
        "            pu = self.model.pu[u]\n",
        "            bu = self.model.bu[u]\n",
        "            qi = self.model.qi\n",
        "            bi = self.model.bi\n",
        "            mu = trainset.global_mean\n",
        "\n",
        "            scores = mu + bu + bi + qi @ pu\n",
        "\n",
        "            known_items = set([j for (j, _) in trainset.ur[u]])\n",
        "            scores[list(known_items)] = -np.inf\n",
        "\n",
        "            top_n_idx = np.argpartition(scores, -n)[-n:]\n",
        "            top_n_idx = top_n_idx[np.argsort(scores[top_n_idx])[::-1]]\n",
        "\n",
        "            return np.array([int(trainset.to_raw_iid(i)) for i in top_n_idx])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0ZU1IOOa1Lg"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, n):\n",
        "    mean_recall = 0.0\n",
        "    mean_ap = 0.0\n",
        "    mean_ndcg = 0.0\n",
        "    mean_novelty = 0.0\n",
        "    mean_diversity = 0.0\n",
        "    for user_id in user_items_test.keys():\n",
        "        rec = ModelWrapper(model).recommend(user_id, user_item_matrix, n)\n",
        "        rel_vector = np.isin(rec, user_items_test[user_id], assume_unique=True).astype(int)\n",
        "        mean_recall += recall_at_k(rel_vector, n)\n",
        "        mean_ap += average_precision_at_k(rel_vector, n)\n",
        "        mean_ndcg += ndcg_at_k(rel_vector, n)\n",
        "        mean_novelty += novelty(rec)\n",
        "        mean_diversity += diversity(rec, n)\n",
        "\n",
        "    mean_recall /= len(user_items_test)\n",
        "    mean_ap /= len(user_items_test)\n",
        "    mean_ndcg /= len(user_items_test)\n",
        "    mean_novelty /= len(user_items_test)\n",
        "    mean_diversity /= len(user_items_test)\n",
        "\n",
        "    return mean_recall, mean_ap, mean_ndcg, mean_novelty, mean_diversity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTbVqvuMa2ha"
      },
      "outputs": [],
      "source": [
        "def plot_results(results, xlabel, title):\n",
        "    df = pd.DataFrame(results, columns=[\"K\", \"Recall\", \"MAP\", \"nDCG\", \"Novelty\", \"Diversity\"])\n",
        "\n",
        "    max_info = np.log2(len(itemset))\n",
        "    df[\"Novelty (norm)\"] = (df[\"Novelty\"] / max_info)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    for metric in [\"Recall\", \"MAP\", \"nDCG\"]:\n",
        "        plt.plot(df[\"K\"], df[metric], marker=\"o\", label=metric)\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    for metric in [\"Novelty (norm)\", \"Diversity\"]:\n",
        "        plt.plot(df[\"K\"], df[metric], marker=\"o\", label=metric)\n",
        "\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 803
        },
        "id": "QqBSRNkta36G",
        "outputId": "f70e7678-ef8c-4543-a8b0-af0a8e8c3bc1"
      },
      "outputs": [],
      "source": [
        "results_rnd = []\n",
        "\n",
        "for i in range(10):\n",
        "    model_rnd = \"random\"\n",
        "    recall_rnd, map_rnd, mndcg_rnd, novelty_rnd, diversity_rnd = evaluate_model(model_rnd, 10)\n",
        "    results_rnd.append([i, recall_rnd, map_rnd, mndcg_rnd, novelty_rnd, diversity_rnd])\n",
        "\n",
        "plot_results(results_rnd, \"Iteración\", \"Rendimiento del Modelo Aleatorio\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TK3TS5DI0nws",
        "outputId": "5730e06e-05d6-4ba9-bae9-7e1724d948a4"
      },
      "outputs": [],
      "source": [
        "results_mp = []\n",
        "\n",
        "# for i in range(10):\n",
        "model_mp = \"most_popular\"\n",
        "recall_mp, map_mp, mndcg_mp, novelty_mp, diversity_mp = evaluate_model(model_mp, 10)\n",
        "results_mp.append([i, recall_mp, map_mp, mndcg_mp, novelty_mp, diversity_mp])\n",
        "\n",
        "# plot_results(results_mp, \"Iteración\", \"Rendimiento del Modelo MostPopular\")\n",
        "print(f\"Recall: {recall_mp}\")\n",
        "print(f\"MAP: {map_mp}\")\n",
        "print(f\"nDCG: {mndcg_mp}\")\n",
        "print(f\"Novelty: {novelty_mp}\")\n",
        "print(f\"Diversity: {diversity_mp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_IIJuFx1r96"
      },
      "source": [
        "# De aqui para abajo no funciona xd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dc8TvtV8a5ga",
        "outputId": "681a5a63-ba5e-4301-d988-fed678a56241"
      },
      "outputs": [],
      "source": [
        "k_values = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "results_iknn = []\n",
        "\n",
        "for k in k_values:\n",
        "    print(k)\n",
        "    model_iknn = implicit.nearest_neighbours.CosineRecommender(K=k)\n",
        "    model_iknn.fit(user_item_matrix, show_progress=False)\n",
        "    recall_iknn, map_iknn, mndcg_iknn, novelty_iknn, diversity_iknn = evaluate_model(model_iknn, 10)\n",
        "    results_iknn.append([k, recall_iknn, map_iknn, mndcg_iknn, novelty_iknn, diversity_iknn])\n",
        "\n",
        "plot_results(results_iknn, \"K (número de vecinos)\", \"Rendimiento del Modelo iKNN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "iC8IID2Da7aP",
        "outputId": "da658f6a-0f5c-427a-c46d-c471610256f5"
      },
      "outputs": [],
      "source": [
        "factor_values = [5, 10, 30, 50, 70, 100, 150, 200, 250, 300]\n",
        "results_svd = []\n",
        "\n",
        "for f in factor_values:\n",
        "    model_svd = SVD(n_factors=f)\n",
        "    model_svd.fit(trainset)\n",
        "    recall_svd, map_svd, mndcg_svd, novelty_svd, diversity_svd = evaluate_model(model_svd, 10)\n",
        "    results_svd.append([f, recall_svd, map_svd, mndcg_svd, novelty_svd, diversity_svd])\n",
        "\n",
        "plot_results(results_svd, \"Número de factores latentes\", \"Rendimiento del Modelo FunkSVD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKn6T4uHa9Cs"
      },
      "outputs": [],
      "source": [
        "factor_values = [5, 10, 30, 50, 70, 100, 150, 200, 250, 300]\n",
        "results_als = []\n",
        "\n",
        "for f in factor_values:\n",
        "    model_als = implicit.als.AlternatingLeastSquares(factors=f)\n",
        "    model_als.fit(user_item_matrix, show_progress=False)\n",
        "    recall_als, map_als, mndcg_als, novelty_als, diversity_als = evaluate_model(model_als, 10)\n",
        "    results_als.append([f, recall_als, map_als, mndcg_als, novelty_als, diversity_als])\n",
        "\n",
        "plot_results(results_als, \"Número de factores latentes\", \"Rendimiento del Modelo ALS\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuZ8S_gMa-mX"
      },
      "outputs": [],
      "source": [
        "factor_values = [5, 10, 30, 50, 70, 100, 150, 200, 250, 300]\n",
        "results_bpr = []\n",
        "\n",
        "for f in tqdm(factor_values):\n",
        "    model_bpr = implicit.bpr.BayesianPersonalizedRanking(factors=f)\n",
        "    model_bpr.fit(user_item_matrix, show_progress=False)\n",
        "    recall_bpr, map_bpr, mndcg_bpr, novelty_bpr, diversity_bpr = evaluate_model(model_bpr, 10)\n",
        "    results_bpr.append([f, recall_bpr, map_bpr, mndcg_bpr, novelty_bpr, diversity_bpr])\n",
        "\n",
        "plot_results(results_bpr, \"Número de factores latentes\", \"Rendimiento del modelo BPR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yp-xoGiIbAU1"
      },
      "outputs": [],
      "source": [
        "def best_performing_by_rank_metrics(results):\n",
        "    best_model = max(results, key=lambda x: x[1] * 0.2 + x[2] * 0.4 + x[3] * 0.4)\n",
        "    return best_model\n",
        "\n",
        "def best_performing_by_novelty_diversity(results):\n",
        "    best_model = max(results, key=lambda x: (x[4] / np.log2(len(itemset))) * 0.5 + x[5] * 0.5)\n",
        "    return best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKodW1c3bBg-"
      },
      "outputs": [],
      "source": [
        "i, recall_rnd, map_rnd, ndcg_rnd, novelty_rnd, diversity_rnd = best_performing_by_rank_metrics(results_rnd)\n",
        "k, recall_iknn, map_iknn, ndcg_iknn, novelty_iknn, diversity_iknn = best_performing_by_rank_metrics(results_iknn)\n",
        "f_svd, recall_svd, map_svd, ndcg_svd, novelty_svd, diversity_svd = best_performing_by_rank_metrics(results_svd)\n",
        "f_als, recall_als, map_als, ndcg_als, novelty_als, diversity_als = best_performing_by_rank_metrics(results_als)\n",
        "f_bpr, recall_bpr, map_bpr, ndcg_bpr, novelty_bpr, diversity_bpr = best_performing_by_rank_metrics(results_bpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwZrAghqbCvi"
      },
      "outputs": [],
      "source": [
        "rows = [\n",
        "    [f\"RANDOM ({i})\", recall_rnd, map_rnd, mndcg_rnd, novelty_rnd, diversity_rnd],\n",
        "    [f\"iKNN ({k})\", recall_iknn, map_iknn, mndcg_iknn, novelty_iknn, diversity_iknn],\n",
        "    [f\"SVD ({f_svd})\", recall_svd, map_svd, mndcg_svd, novelty_svd, diversity_svd],\n",
        "    [f\"ALS ({f_als})\", recall_als, map_als, mndcg_als, novelty_als, diversity_als],\n",
        "    [f\"BPR ({f_bpr})\", recall_bpr, map_bpr, mndcg_bpr, novelty_bpr, diversity_bpr],\n",
        "]\n",
        "print(tabulate(rows, headers=[\"Modelo\", \"RECALL\", \"MAP\", \"MNDCG\", \"NOVELTY\", \"DIVERSITY\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzLL2J-EbNbd"
      },
      "outputs": [],
      "source": [
        "i, recall_rnd, map_rnd, ndcg_rnd, novelty_rnd, diversity_rnd = best_performing_by_novelty_diversity(results_rnd)\n",
        "k, recall_iknn, map_iknn, ndcg_iknn, novelty_iknn, diversity_iknn = best_performing_by_novelty_diversity(results_iknn)\n",
        "f_svd, recall_svd, map_svd, ndcg_svd, novelty_svd, diversity_svd = best_performing_by_novelty_diversity(results_svd)\n",
        "f_als, recall_als, map_als, ndcg_als, novelty_als, diversity_als = best_performing_by_novelty_diversity(results_als)\n",
        "f_bpr, recall_bpr, map_bpr, ndcg_bpr, novelty_bpr, diversity_bpr = best_performing_by_novelty_diversity(results_bpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVHaQrATbOn1"
      },
      "outputs": [],
      "source": [
        "rows = [\n",
        "    [f\"RANDOM ({i})\", recall_rnd, map_rnd, mndcg_rnd, novelty_rnd, diversity_rnd],\n",
        "    [f\"iKNN ({k})\", recall_iknn, map_iknn, mndcg_iknn, novelty_iknn, diversity_iknn],\n",
        "    [f\"SVD ({f_svd})\", recall_svd, map_svd, mndcg_svd, novelty_svd, diversity_svd],\n",
        "    [f\"ALS ({f_als})\", recall_als, map_als, mndcg_als, novelty_als, diversity_als],\n",
        "    [f\"BPR ({f_bpr})\", recall_bpr, map_bpr, mndcg_bpr, novelty_bpr, diversity_bpr],\n",
        "]\n",
        "print(tabulate(rows, headers=[\"Modelo\", \"RECALL\", \"MAP\", \"MNDCG\", \"NOVELTY\", \"DIVERSITY\"], tablefmt=\"grid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "__HOGtquoh5I",
        "outputId": "74d723a7-60eb-4a85-e42a-3de9522bfcfd"
      },
      "outputs": [],
      "source": [
        "# --- INICIO: Nuevo código para Modelo y Evaluación ---\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "if train_file.empty or validation_file.empty:\n",
        "    print(\"Datos de entrenamiento o validación vacíos. No se puede entrenar el modelo.\")\n",
        "else:\n",
        "    print(\"Iniciando entrenamiento y evaluación del modelo de recomendación...\")\n",
        "\n",
        "    # Definir item_popularity (necesario para la métrica 'novelty')\n",
        "    # Se calcula sobre el set de entrenamiento\n",
        "    item_popularity = train_file['item_id'].value_counts(normalize=True).to_dict()\n",
        "\n",
        "    # 1. Preparar datos para Factorización de Matriz\n",
        "    # Mapear user_id y item_id a índices de matriz (enteros de 0 a N-1)\n",
        "    users = sorted(train_file['user_id'].unique())\n",
        "    items = sorted(train_file['item_id'].unique())\n",
        "\n",
        "    user_mapper = {user: i for i, user in enumerate(users)}\n",
        "    item_mapper = {item: i for i, item in enumerate(items)}\n",
        "\n",
        "    # Mapeo inverso para traducir índices de vuelta a item_id\n",
        "    item_inv_mapper = {i: item for item, i in item_mapper.items()}\n",
        "\n",
        "    # Mapear los dataframes a los nuevos índices\n",
        "    rows = train_file['user_id'].map(user_mapper)\n",
        "    cols = train_file['item_id'].map(item_mapper)\n",
        "    ratings = train_file['rating']\n",
        "\n",
        "    # Crear la matriz dispersa (sparse matrix) de interacciones\n",
        "    R_train = matrix\n",
        "    print(f\"Creada matriz usuario-item de dimensiones: {R_train.shape}\")\n",
        "\n",
        "    # 2. Entrenar el modelo NMF\n",
        "    N_FACTORS = 50  # Número de factores latentes (hiperparámetro)\n",
        "    K = 10          # Top-K para las métricas\n",
        "\n",
        "    model = NMF(\n",
        "        n_components=N_FACTORS,\n",
        "        init='random',\n",
        "        random_state=42,\n",
        "        max_iter=200,  # 200 iteraciones es común, 500 es más robusto\n",
        "        solver='cd',   # 'cd' (Coordinate Descent) es más rápido para datos dispersos\n",
        "        tol=1e-4\n",
        "    )\n",
        "\n",
        "    print(f\"Entrenando modelo NMF con {N_FACTORS} factores...\")\n",
        "    W = model.fit_transform(R_train)  # Matriz de factores de Usuario (Users x Factors)\n",
        "    H = model.components_           # Matriz de factores de Item (Factors x Items)\n",
        "    print(\"Entrenamiento completado.\")\n",
        "\n",
        "    # 3. Obtener la matriz de predicciones completas\n",
        "    # R_hat = W · H\n",
        "    R_hat = np.dot(W, H)\n",
        "\n",
        "    # 4. Evaluar el modelo\n",
        "    # Preparar diccionarios de \"ground truth\" (validación) y \"vistos\" (entrenamiento)\n",
        "    test_users_val = validation_file['user_id'].unique()\n",
        "    true_items_val = validation_file.groupby('user_id')['item_id'].apply(set)\n",
        "    train_items_all = train_file.groupby('user_id')['item_id'].apply(set)\n",
        "\n",
        "    all_precision, all_map, all_recall, all_ndcg, all_novelty, all_diversity = [], [], [], [], [], []\n",
        "\n",
        "    print(f\"Evaluando métricas para {len(test_users_val)} usuarios del set de validación...\")\n",
        "\n",
        "    for user_id in test_users_val:\n",
        "        # Solo podemos evaluar usuarios que estaban en el set de entrenamiento\n",
        "        if user_id not in user_mapper:\n",
        "            continue\n",
        "\n",
        "        u_idx = user_mapper[user_id]\n",
        "\n",
        "        # Obtener todas las predicciones de rating para este usuario\n",
        "        user_preds = R_hat[u_idx, :]\n",
        "\n",
        "        # Filtrar animes que el usuario YA VIO en el set de entrenamiento\n",
        "        # No queremos re-recomendar lo que ya vieron\n",
        "        train_items_user = train_items_all.get(user_id, set())\n",
        "        train_item_indices = [item_mapper[i] for i in train_items_user if i in item_mapper]\n",
        "\n",
        "        user_preds[train_item_indices] = -1.0  # Poner score bajo para que no salgan en el top-K\n",
        "\n",
        "        # Obtener los Top-K índices de items\n",
        "        top_k_indices = np.argsort(user_preds)[::-1][:K]\n",
        "\n",
        "        # Convertir índices de vuelta a item_ids\n",
        "        top_k_items = [item_inv_mapper[i] for i in top_k_indices if i in item_inv_mapper]\n",
        "\n",
        "        # Obtener los items relevantes (ground truth) del set de validación\n",
        "        true_positives = true_items_val.get(user_id, set())\n",
        "        if not true_positives:\n",
        "            continue # Usuario no tiene items en el set de validación\n",
        "\n",
        "        # Construir el vector de relevancia 'r'\n",
        "        # r[i] = 1 si el item recomendado 'i' está en el set de validación, 0 si no\n",
        "        r = [1 if item in true_positives else 0 for item in top_k_items]\n",
        "\n",
        "        # Si no hay ningún ítem relevante en el Top-K, las métricas son 0 (o las saltamos)\n",
        "        if sum(r) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calcular y almacenar métricas\n",
        "        all_precision.append(precision_at_k(r, K))\n",
        "        all_map.append(average_precision_at_k(r, K))\n",
        "        all_recall.append(recall_at_k(r, K))\n",
        "        all_ndcg.append(ndcg_at_k(r, K))\n",
        "        all_novelty.append(novelty(top_k_items))\n",
        "        all_diversity.append(diversity(top_k_items, K))\n",
        "\n",
        "    # 5. Imprimir resultados promedio\n",
        "    print(\"\\n--- Métricas de Evaluación Promedio @K=10 ---\")\n",
        "    if len(all_ndcg) > 0:\n",
        "        print(f\"Usuarios evaluados (con items relevantes en Top-K): {len(all_ndcg)}\")\n",
        "        print(f\"Mean Precision@{K}:       {np.mean(all_precision):.4f}\")\n",
        "        print(f\"Mean Avg Precision (MAP)@{K}: {np.mean(all_map):.4f}\")\n",
        "        print(f\"Mean Recall@{K}:          {np.mean(all_recall):.4f}\")\n",
        "        print(f\"Mean NDCG@{K}:            {np.mean(all_ndcg):.4f}\")\n",
        "        print(f\"Mean Novelty@{K}:         {np.mean(all_novelty):.4f}\")\n",
        "        print(f\"Mean Diversity@{K}:       {np.mean(all_diversity):.4f}\")\n",
        "    else:\n",
        "        print(\"No se pudo evaluar ningún usuario (posiblemente no hubo 'hits' en el Top-K).\")\n",
        "\n",
        "# --- FIN: Nuevo código ---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GTptUTcpJm6",
        "outputId": "bf98b460-b475-4fc5-9f9e-0f862cff39c4"
      },
      "outputs": [],
      "source": [
        "# --- INICIO: Nuevo código para Modelo y Evaluación ---\n",
        "\n",
        "if train_file.empty or validation_file.empty:\n",
        "    print(\"Datos de entrenamiento o validación vacíos. No se puede entrenar el modelo.\")\n",
        "else:\n",
        "    print(\"Iniciando entrenamiento y evaluación del modelo de recomendación...\")\n",
        "\n",
        "    # Definir item_popularity (necesario para la métrica 'novelty')\n",
        "    # Se calcula sobre el set de entrenamiento\n",
        "    item_popularity = train_file['item_id'].value_counts(normalize=True).to_dict()\n",
        "\n",
        "    # 1. Preparar datos para Factorización de Matriz\n",
        "    # Mapear user_id y item_id a índices de matriz (enteros de 0 a N-1)\n",
        "    users = sorted(train_file['user_id'].unique())\n",
        "    items = sorted(train_file['item_id'].unique())\n",
        "\n",
        "    user_mapper = {user: i for i, user in enumerate(users)}\n",
        "    item_mapper = {item: i for i, item in enumerate(items)}\n",
        "\n",
        "    # Mapeo inverso para traducir índices de vuelta a item_id\n",
        "    item_inv_mapper = {i: item for item, i in item_mapper.items()}\n",
        "\n",
        "    # Mapear los dataframes a los nuevos índices\n",
        "    rows = train_file['user_id'].map(user_mapper)\n",
        "    cols = train_file['item_id'].map(item_mapper)\n",
        "    ratings = train_file['rating']\n",
        "\n",
        "    # Crear la matriz dispersa (sparse matrix) de interacciones\n",
        "    R_train = sp.csr_matrix((ratings, (rows, cols)), shape=(len(users), len(items)))\n",
        "    print(f\"Creada matriz usuario-item de dimensiones: {R_train.shape}\")\n",
        "\n",
        "    # 2. Entrenar el modelo NMF\n",
        "    N_FACTORS = 50  # Número de factores latentes (hiperparámetro)\n",
        "    K = 10          # Top-K para las métricas\n",
        "\n",
        "    model = NMF(\n",
        "        n_components=N_FACTORS,\n",
        "        init='random',\n",
        "        random_state=42,\n",
        "        max_iter=200,  # 200 iteraciones es común, 500 es más robusto\n",
        "        solver='cd',   # 'cd' (Coordinate Descent) es más rápido para datos dispersos\n",
        "        tol=1e-4\n",
        "    )\n",
        "\n",
        "    print(f\"Entrenando modelo NMF con {N_FACTORS} factores...\")\n",
        "    W = model.fit_transform(R_train)  # Matriz de factores de Usuario (Users x Factors)\n",
        "    H = model.components_           # Matriz de factores de Item (Factors x Items)\n",
        "    print(\"Entrenamiento completado.\")\n",
        "\n",
        "    # 3. Obtener la matriz de predicciones completas\n",
        "    # R_hat = W · H\n",
        "    R_hat = np.dot(W, H)\n",
        "\n",
        "    # 4. Evaluar el modelo\n",
        "    # Preparar diccionarios de \"ground truth\" (validación) y \"vistos\" (entrenamiento)\n",
        "    test_users_val = validation_file['user_id'].unique()\n",
        "    true_items_val = validation_file.groupby('user_id')['item_id'].apply(set)\n",
        "    train_items_all = train_file.groupby('user_id')['item_id'].apply(set)\n",
        "\n",
        "    all_precision, all_map, all_recall, all_ndcg, all_novelty, all_diversity = [], [], [], [], [], []\n",
        "\n",
        "    print(f\"Evaluando métricas para {len(test_users_val)} usuarios del set de validación...\")\n",
        "\n",
        "    for user_id in test_users_val:\n",
        "        # Solo podemos evaluar usuarios que estaban en el set de entrenamiento\n",
        "        if user_id not in user_mapper:\n",
        "            continue\n",
        "\n",
        "        u_idx = user_mapper[user_id]\n",
        "\n",
        "        # Obtener todas las predicciones de rating para este usuario\n",
        "        user_preds = R_hat[u_idx, :]\n",
        "\n",
        "        # Filtrar animes que el usuario YA VIO en el set de entrenamiento\n",
        "        # No queremos re-recomendar lo que ya vieron\n",
        "        train_items_user = train_items_all.get(user_id, set())\n",
        "        train_item_indices = [item_mapper[i] for i in train_items_user if i in item_mapper]\n",
        "\n",
        "        user_preds[train_item_indices] = -1.0  # Poner score bajo para que no salgan en el top-K\n",
        "\n",
        "        # Obtener los Top-K índices de items\n",
        "        top_k_indices = np.argsort(user_preds)[::-1][:K]\n",
        "\n",
        "        # Convertir índices de vuelta a item_ids\n",
        "        top_k_items = [item_inv_mapper[i] for i in top_k_indices if i in item_inv_mapper]\n",
        "\n",
        "        # Obtener los items relevantes (ground truth) del set de validación\n",
        "        true_positives = true_items_val.get(user_id, set())\n",
        "        if not true_positives:\n",
        "            continue # Usuario no tiene items en el set de validación\n",
        "\n",
        "        # Construir el vector de relevancia 'r'\n",
        "        # r[i] = 1 si el item recomendado 'i' está en el set de validación, 0 si no\n",
        "        r = [1 if item in true_positives else 0 for item in top_k_items]\n",
        "\n",
        "        # Si no hay ningún ítem relevante en el Top-K, las métricas son 0 (o las saltamos)\n",
        "        if sum(r) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calcular y almacenar métricas\n",
        "        all_precision.append(precision_at_k(r, K))\n",
        "        all_map.append(average_precision_at_k(r, K))\n",
        "        all_recall.append(recall_at_k(r, K))\n",
        "        all_ndcg.append(ndcg_at_k(r, K))\n",
        "        all_novelty.append(novelty(top_k_items))\n",
        "        all_diversity.append(diversity(top_k_items, K))\n",
        "\n",
        "    # 5. Imprimir resultados promedio\n",
        "    print(\"\\n--- Métricas de Evaluación Promedio @K=10 ---\")\n",
        "    if len(all_ndcg) > 0:\n",
        "        print(f\"Usuarios evaluados (con items relevantes en Top-K): {len(all_ndcg)}\")\n",
        "        print(f\"Mean Precision@{K}:       {np.mean(all_precision):.4f}\")\n",
        "        print(f\"Mean Avg Precision (MAP)@{K}: {np.mean(all_map):.4f}\")\n",
        "        print(f\"Mean Recall@{K}:          {np.mean(all_recall):.4f}\")\n",
        "        print(f\"Mean NDCG@{K}:            {np.mean(all_ndcg):.4f}\")\n",
        "        print(f\"Mean Novelty@{K}:         {np.mean(all_novelty):.4f}\")\n",
        "        print(f\"Mean Diversity@{K}:       {np.mean(all_diversity):.4f}\")\n",
        "    else:\n",
        "        print(\"No se pudo evaluar ningún usuario (posiblemente no hubo 'hits' en el Top-K).\")\n",
        "\n",
        "# --- FIN: Nuevo código ---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "proyecto",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
