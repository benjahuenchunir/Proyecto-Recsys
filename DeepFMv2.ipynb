{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from deepctr_torch.inputs import SparseFeat, DenseFeat, VarLenSparseFeat, get_feature_names\n",
    "from deepctr_torch.models import DeepFM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# DeepFM with Side Information (Genres, Score, Popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "animes = pd.read_csv('clean_data/animes.csv')\n",
    "df_train = pd.read_csv(\"train\", sep=\",\", names=[\"userid\", \"itemid\", \"rating\"], header=None)\n",
    "df_test = pd.read_csv(\"test\", sep=\",\", names=[\"userid\", \"itemid\", \"rating\"], header=None)\n",
    "\n",
    "# Binarize ratings (Target)\n",
    "df_train['target'] = [1 if x >= 5 else 0 for x in df_train['rating']]\n",
    "df_train = df_train.dropna(subset=['itemid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feature_eng_animes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Feature Engineering on Animes Metadata\n",
    "import ast  # <--- NEW IMPORT\n",
    "\n",
    "# > Clean Numerical Features\n",
    "# Handle 'Unknown' in episodes\n",
    "animes['episodes'] = pd.to_numeric(animes['episodes'], errors='coerce').fillna(1)\n",
    "animes['score'] = pd.to_numeric(animes['score'], errors='coerce').fillna(animes['score'].mean())\n",
    "animes['members'] = pd.to_numeric(animes['members'], errors='coerce').fillna(0)\n",
    "\n",
    "# > Extract Year from 'aired'\n",
    "# Format is usually 'Month Day, Year' or 'Year'. We extract the first 4 digits.\n",
    "animes['year'] = animes['aired'].str.extract(r'(\\d{4})').fillna('2000')\n",
    "animes['year'] = animes['year'].astype(int)\n",
    "\n",
    "# > Process Genres (Multi-Label) <--- CHANGED SECTION\n",
    "# The csv has genres like: \"['Comedy', 'Sports']\" (Stringified list)\n",
    "# We use ast.literal_eval to safely convert that string into a real python list.\n",
    "def parse_genre(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "animes['genre'] = animes['genre'].fillna('[]').apply(parse_genre)\n",
    "\n",
    "# Create Genre Vocabulary\n",
    "genre_list = set()\n",
    "for genres in animes['genre']:\n",
    "    genre_list.update(genres)\n",
    "    \n",
    "genre_to_idx = {genre: i+1 for i, genre in enumerate(genre_list)} # 0 is padding\n",
    "genre_vocab_size = len(genre_to_idx) + 1\n",
    "\n",
    "# Encode Genres into integer lists\n",
    "def encode_genres(genre_list):\n",
    "    return [genre_to_idx.get(g, 0) for g in genre_list]\n",
    "\n",
    "animes['genre_encoded'] = animes['genre'].apply(encode_genres)\n",
    "\n",
    "# Pad genres to fixed length (e.g., max 4 genres per anime)\n",
    "MAX_GENRE_LEN = 4\n",
    "genre_padded = pad_sequences(animes['genre_encoded'], maxlen=MAX_GENRE_LEN, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "prep_input",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Merge Metadata into Interaction Data\n",
    "data = pd.concat([df_train, df_test], axis=0, ignore_index=True)\n",
    "\n",
    "# Merge side info\n",
    "data = data.merge(animes[['uid', 'score', 'members', 'episodes', 'year']], left_on='itemid', right_on='uid', how='left')\n",
    "\n",
    "# Fill missing metadata (for items in train/test but not in animes.csv)\n",
    "data['score'] = data['score'].fillna(data['score'].mean())\n",
    "data['members'] = data['members'].fillna(0)\n",
    "data['episodes'] = data['episodes'].fillna(1)\n",
    "data['year'] = data['year'].fillna(2000)\n",
    "\n",
    "# 4. Encode ID Features\n",
    "lbe_user = LabelEncoder()\n",
    "data['userid_enc'] = lbe_user.fit_transform(data['userid'])\n",
    "\n",
    "lbe_item = LabelEncoder()\n",
    "data['itemid_enc'] = lbe_item.fit_transform(data['itemid'])\n",
    "\n",
    "lbe_year = LabelEncoder()\n",
    "data['year_enc'] = lbe_year.fit_transform(data['year'])\n",
    "\n",
    "# 5. Normalize Dense Features\n",
    "mms = MinMaxScaler(feature_range=(0, 1))\n",
    "data[['score', 'members', 'episodes']] = mms.fit_transform(data[['score', 'members', 'episodes']])\n",
    "\n",
    "# 6. Prepare Genre Input Column\n",
    "# We need to map the merged data back to the padded genre array\n",
    "# Create a mapping from itemid -> padded_genre_row\n",
    "item_id_to_genre = dict(zip(animes['uid'], genre_padded))\n",
    "\n",
    "# Look up genres for every row in data\n",
    "# Use a default [0,0,0,0] for missing items\n",
    "default_genre = np.zeros(MAX_GENRE_LEN, dtype=int)\n",
    "genre_feature_data = np.array([item_id_to_genre.get(iid, default_genre) for iid in data['itemid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "define_feats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Define Feature Columns for DeepFM\n",
    "# FIX: All embedding_dim must be equal (e.g., 16) for DeepFM interaction layers.\n",
    "\n",
    "fixlen_feature_columns = [\n",
    "    SparseFeat(\"userid_enc\", vocabulary_size=data['userid_enc'].max() + 1, embedding_dim=16),\n",
    "    SparseFeat(\"itemid_enc\", vocabulary_size=data['itemid_enc'].max() + 1, embedding_dim=16),\n",
    "    # CHANGED: 8 -> 16\n",
    "    SparseFeat(\"year_enc\", vocabulary_size=data['year_enc'].max() + 1, embedding_dim=16), \n",
    "    DenseFeat(\"score\", 1),\n",
    "    DenseFeat(\"members\", 1),\n",
    "    DenseFeat(\"episodes\", 1)\n",
    "]\n",
    "\n",
    "# Variable length feature for Genres\n",
    "varlen_feature_columns = [\n",
    "    # CHANGED: 8 -> 16\n",
    "    VarLenSparseFeat(SparseFeat('genre', vocabulary_size=genre_vocab_size, embedding_dim=16),\n",
    "                     maxlen=MAX_GENRE_LEN, combiner='mean')\n",
    "]\n",
    "\n",
    "linear_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "dnn_feature_columns = fixlen_feature_columns + varlen_feature_columns\n",
    "feature_names = get_feature_names(linear_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "split_train_test",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Split Train/Test and Generate Dictionaries\n",
    "train_idx = slice(0, len(df_train))\n",
    "test_idx = slice(len(df_train), len(data))\n",
    "\n",
    "def make_input_dict(idx):\n",
    "    input_dict = {name: data.iloc[idx][name].values for name in feature_names if 'genre' not in name}\n",
    "    # Add the genre array separately\n",
    "    input_dict['genre'] = genre_feature_data[idx]\n",
    "    return input_dict\n",
    "\n",
    "train_model_input = make_input_dict(train_idx)\n",
    "test_model_input = make_input_dict(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "train_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Train on 139689 samples, validate on 0 samples, 546 steps per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:03, 164.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3s - loss:  0.3492 - binary_crossentropy:  0.3493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:02, 192.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "2s - loss:  0.3328 - binary_crossentropy:  0.3328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:02, 183.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "2s - loss:  0.3034 - binary_crossentropy:  0.3033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:02, 193.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5\n",
      "2s - loss:  0.2784 - binary_crossentropy:  0.2783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "546it [00:02, 189.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5\n",
      "2s - loss:  0.2640 - binary_crossentropy:  0.2638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Train Model\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = DeepFM(linear_feature_columns, dnn_feature_columns, task='binary', device=device)\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=['binary_crossentropy'])\n",
    "\n",
    "# Increased epochs to 5 for better convergence with new features\n",
    "history = model.fit(train_model_input, df_train['target'].values, batch_size=256, epochs=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "inference_logic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Inference Logic with Side Info ---\n",
    "\n",
    "# Pre-calculate unique item features to avoid re-processing in the loop\n",
    "unique_items = data[['itemid_enc', 'itemid', 'score', 'members', 'episodes', 'year_enc']].drop_duplicates('itemid_enc')\n",
    "unique_items = unique_items.sort_values('itemid_enc').reset_index(drop=True)\n",
    "\n",
    "# Helper arrays for fast lookup\n",
    "all_items_enc = unique_items['itemid_enc'].values\n",
    "all_item_scores = unique_items['score'].values\n",
    "all_item_members = unique_items['members'].values\n",
    "all_item_episodes = unique_items['episodes'].values\n",
    "all_item_years = unique_items['year_enc'].values\n",
    "\n",
    "# Genre lookup by encoded item id\n",
    "# unique_items['itemid'] maps to original ID. We use that to get genre array.\n",
    "all_item_genres = np.array([item_id_to_genre.get(iid, default_genre) for iid in unique_items['itemid']])\n",
    "\n",
    "enc_to_raw_item = {enc: raw for enc, raw in zip(unique_items['itemid_enc'], unique_items['itemid'])}\n",
    "\n",
    "def get_recommendations(user_id, n):\n",
    "    try:\n",
    "        user_enc = lbe_user.transform([user_id])[0]\n",
    "    except ValueError:\n",
    "        return np.array([])\n",
    "\n",
    "    # Broadcast User ID\n",
    "    count = len(all_items_enc)\n",
    "    user_enc_col = np.full(count, user_enc)\n",
    "    \n",
    "    # Construct Input with ALL features\n",
    "    pred_input = {\n",
    "        \"userid_enc\": user_enc_col,\n",
    "        \"itemid_enc\": all_items_enc,\n",
    "        \"year_enc\": all_item_years,\n",
    "        \"score\": all_item_scores,\n",
    "        \"members\": all_item_members,\n",
    "        \"episodes\": all_item_episodes,\n",
    "        \"genre\": all_item_genres\n",
    "    }\n",
    "    \n",
    "    # Predict\n",
    "    preds = model.predict(pred_input, batch_size=4096).flatten()\n",
    "    \n",
    "    # Rank\n",
    "    top_indices = preds.argsort()[-n:][::-1]\n",
    "    top_enc_items = all_items_enc[top_indices]\n",
    "    \n",
    "    return np.array([enc_to_raw_item[i] for i in top_enc_items])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluando usuarios: 100%|██████████| 18591/18591 [08:32<00:00, 36.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Métricas Globales de Evaluación ---\n",
      "{\n",
      "  \"mean_recall\": 0.004525695230155422,\n",
      "  \"mean_precision\": 0.0004525695230155422,\n",
      "  \"mean_ap (MAP)\": 0.0015170011314238076,\n",
      "  \"mean_ndcg\": 0.0022015454557601315,\n",
      "  \"mean_novelty\": 9.765866344968858,\n",
      "  \"mean_diversity\": 0.0,\n",
      "  \"num_users_evaluated\": 16793\n",
      "}\n",
      "\n",
      "--- Reporte de Fairness (Disparidad de Grupo) ---\n",
      "{\n",
      "  \"delta_threshold\": 0.05,\n",
      "  \"is_biased_recall\": 0,\n",
      "  \"is_biased_precision\": 0,\n",
      "  \"group_averages\": {\n",
      "    \"Male\": {\n",
      "      \"recall (Cobertura)\": 0.0039657950179700086,\n",
      "      \"precision (Tasa Aceptaci\\u00f3n)\": 0.0003965795017970009,\n",
      "      \"count\": 8069\n",
      "    },\n",
      "    \"NaN\": {\n",
      "      \"recall (Cobertura)\": 0.004893077201884741,\n",
      "      \"precision (Tasa Aceptaci\\u00f3n)\": 0.0004893077201884741,\n",
      "      \"count\": 5518\n",
      "    },\n",
      "    \"Non-Binary\": {\n",
      "      \"recall (Cobertura)\": 0.0,\n",
      "      \"precision (Tasa Aceptaci\\u00f3n)\": 0.0,\n",
      "      \"count\": 137\n",
      "    },\n",
      "    \"Female\": {\n",
      "      \"recall (Cobertura)\": 0.005539263603779733,\n",
      "      \"precision (Tasa Aceptaci\\u00f3n)\": 0.0005539263603779734,\n",
      "      \"count\": 3069\n",
      "    }\n",
      "  },\n",
      "  \"disparity_reports\": [\n",
      "    {\n",
      "      \"pair\": [\n",
      "        \"Male\",\n",
      "        NaN\n",
      "      ],\n",
      "      \"recall_disparity\": 0.0009272821839147322,\n",
      "      \"precision_disparity\": 9.272821839147318e-05,\n",
      "      \"biased_recall (Cobertura)\": 0,\n",
      "      \"biased_precision (Tasa Aceptaci\\u00f3n)\": 0\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        \"Male\",\n",
      "        \"Non-Binary\"\n",
      "      ],\n",
      "      \"recall_disparity\": 0.0039657950179700086,\n",
      "      \"precision_disparity\": 0.0003965795017970009,\n",
      "      \"biased_recall (Cobertura)\": 0,\n",
      "      \"biased_precision (Tasa Aceptaci\\u00f3n)\": 0\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        \"Male\",\n",
      "        \"Female\"\n",
      "      ],\n",
      "      \"recall_disparity\": 0.0015734685858097243,\n",
      "      \"precision_disparity\": 0.00015734685858097248,\n",
      "      \"biased_recall (Cobertura)\": 0,\n",
      "      \"biased_precision (Tasa Aceptaci\\u00f3n)\": 0\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        NaN,\n",
      "        \"Non-Binary\"\n",
      "      ],\n",
      "      \"recall_disparity\": 0.004893077201884741,\n",
      "      \"precision_disparity\": 0.0004893077201884741,\n",
      "      \"biased_recall (Cobertura)\": 0,\n",
      "      \"biased_precision (Tasa Aceptaci\\u00f3n)\": 0\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        NaN,\n",
      "        \"Female\"\n",
      "      ],\n",
      "      \"recall_disparity\": 0.0006461864018949921,\n",
      "      \"precision_disparity\": 6.46186401894993e-05,\n",
      "      \"biased_recall (Cobertura)\": 0,\n",
      "      \"biased_precision (Tasa Aceptaci\\u00f3n)\": 0\n",
      "    },\n",
      "    {\n",
      "      \"pair\": [\n",
      "        \"Non-Binary\",\n",
      "        \"Female\"\n",
      "      ],\n",
      "      \"recall_disparity\": 0.005539263603779733,\n",
      "      \"precision_disparity\": 0.0005539263603779734,\n",
      "      \"biased_recall (Cobertura)\": 0,\n",
      "      \"biased_precision (Tasa Aceptaci\\u00f3n)\": 0\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Recall Global: 0.0045\n",
      "MAP Global: 0.0015\n",
      "¿Es sesgado (Recall)?: 0\n",
      "¿Es sesgado (Precision)?: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evaluate import get_metrics\n",
    "\n",
    "# Metrics setup (same as before)\n",
    "item_interaction_counts = df_train['itemid'].value_counts()\n",
    "user_count = df_train['userid'].nunique()\n",
    "item_popularity = (item_interaction_counts / user_count).to_dict()\n",
    "metadata = animes[['uid', 'genre']]\n",
    "item_categories: dict[int, set[str | None]] = {}\n",
    "for row in metadata.itertuples():\n",
    "    item_categories[int(row[1]) if hasattr(row[1], 'is_integer') and row[1].is_integer() else row[1]] = set(map(lambda i: i.strip(), row[2].split(','))) if isinstance(row[2], str) else set()\n",
    "\n",
    "user_items_test = {}\n",
    "for row in df_test.itertuples():\n",
    "    if row.userid not in user_items_test:\n",
    "        user_items_test[row.userid] = []\n",
    "    user_items_test[row.userid].append(row.itemid)\n",
    "\n",
    "get_metrics(user_items_test, item_popularity, item_categories, get_recommendations, k=10, delta=0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
